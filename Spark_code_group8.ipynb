{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process csv and create Deltatable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/ubuntu/spark-3.3.2-bin-without-hadoop/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/ubuntu/.ivy2/cache\n",
      "The jars for the packages stored in: /home/ubuntu/.ivy2/jars\n",
      "io.delta#delta-core_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-f451d78f-c8c4-4ebc-b747-e01f1ec32b1b;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-core_2.12;2.2.0 in central\n",
      "\tfound io.delta#delta-storage;2.2.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.8 in central\n",
      ":: resolution report :: resolve 535ms :: artifacts dl 22ms\n",
      "\t:: modules in use:\n",
      "\tio.delta#delta-core_2.12;2.2.0 from central in [default]\n",
      "\tio.delta#delta-storage;2.2.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.8 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-f451d78f-c8c4-4ebc-b747-e01f1ec32b1b\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/16ms)\n",
      "2023-04-26 11:32:18,702 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "2023-04-26 11:32:19,394 INFO spark.SparkContext: Running Spark version 3.3.2\n",
      "2023-04-26 11:32:19,453 INFO resource.ResourceUtils: ==============================================================\n",
      "2023-04-26 11:32:19,454 INFO resource.ResourceUtils: No custom resources configured for spark.driver.\n",
      "2023-04-26 11:32:19,455 INFO resource.ResourceUtils: ==============================================================\n",
      "2023-04-26 11:32:19,455 INFO spark.SparkContext: Submitted application: MyApp\n",
      "2023-04-26 11:32:19,514 INFO resource.ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1536, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
      "2023-04-26 11:32:19,537 INFO resource.ResourceProfile: Limiting resource is cpus at 1 tasks per executor\n",
      "2023-04-26 11:32:19,540 INFO resource.ResourceProfileManager: Added ResourceProfile id: 0\n",
      "2023-04-26 11:32:19,710 INFO spark.SecurityManager: Changing view acls to: ubuntu\n",
      "2023-04-26 11:32:19,711 INFO spark.SecurityManager: Changing modify acls to: ubuntu\n",
      "2023-04-26 11:32:19,711 INFO spark.SecurityManager: Changing view acls groups to: \n",
      "2023-04-26 11:32:19,712 INFO spark.SecurityManager: Changing modify acls groups to: \n",
      "2023-04-26 11:32:19,713 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(ubuntu); groups with view permissions: Set(); users  with modify permissions: Set(ubuntu); groups with modify permissions: Set()\n",
      "2023-04-26 11:32:20,470 INFO util.Utils: Successfully started service 'sparkDriver' on port 44939.\n",
      "2023-04-26 11:32:20,592 INFO spark.SparkEnv: Registering MapOutputTracker\n",
      "2023-04-26 11:32:20,704 INFO spark.SparkEnv: Registering BlockManagerMaster\n",
      "2023-04-26 11:32:20,790 INFO storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "2023-04-26 11:32:20,791 INFO storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "2023-04-26 11:32:20,888 INFO spark.SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "2023-04-26 11:32:21,028 INFO storage.DiskBlockManager: Created local directory at /tmp/blockmgr-26d168ef-6bfa-4747-96c0-a4923d4b9881\n",
      "2023-04-26 11:32:21,164 INFO memory.MemoryStore: MemoryStore started with capacity 3.6 GiB\n",
      "2023-04-26 11:32:21,246 INFO spark.SparkEnv: Registering OutputCommitCoordinator\n",
      "2023-04-26 11:32:21,492 INFO util.log: Logging initialized @9790ms to org.sparkproject.jetty.util.log.Slf4jLog\n",
      "2023-04-26 11:32:21,821 INFO server.Server: jetty-9.4.48.v20220622; built: 2022-06-21T20:42:25.880Z; git: 6b67c5719d1f4371b33655ff2d047d24e171e49a; jvm 1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09\n",
      "2023-04-26 11:32:21,909 INFO server.Server: Started @10210ms\n",
      "2023-04-26 11:32:22,011 INFO server.AbstractConnector: Started ServerConnector@2e5eda3b{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}\n",
      "2023-04-26 11:32:22,011 INFO util.Utils: Successfully started service 'SparkUI' on port 4040.\n",
      "2023-04-26 11:32:22,049 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@12fee641{/,null,AVAILABLE,@Spark}\n",
      "2023-04-26 11:32:23,383 INFO client.RMProxy: Connecting to ResourceManager at namenode/192.168.1.93:8032\n",
      "2023-04-26 11:32:25,289 INFO conf.Configuration: resource-types.xml not found\n",
      "2023-04-26 11:32:25,290 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2023-04-26 11:32:25,327 INFO yarn.Client: Verifying our application has not requested more than the maximum memory capability of the cluster (8192 MB per container)\n",
      "2023-04-26 11:32:25,328 INFO yarn.Client: Will allocate AM container, with 896 MB memory including 384 MB overhead\n",
      "2023-04-26 11:32:25,328 INFO yarn.Client: Setting up container launch context for our AM\n",
      "2023-04-26 11:32:25,340 INFO yarn.Client: Setting up the launch environment for our AM container\n",
      "2023-04-26 11:32:25,357 INFO yarn.Client: Preparing resources for our AM container\n",
      "2023-04-26 11:32:25,441 WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n",
      "2023-04-26 11:32:40,278 INFO yarn.Client: Uploading resource file:/tmp/spark-9107ce6b-4f7a-4db3-8734-eb2c972b755f/__spark_libs__7520608614242200267.zip -> hdfs://namenode:9000/user/ubuntu/.sparkStaging/application_1682506622048_0002/__spark_libs__7520608614242200267.zip\n",
      "2023-04-26 11:32:41,009 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2023-04-26 11:32:42,628 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2023-04-26 11:32:43,400 INFO yarn.Client: Uploading resource file:/home/ubuntu/.ivy2/jars/io.delta_delta-core_2.12-2.2.0.jar -> hdfs://namenode:9000/user/ubuntu/.sparkStaging/application_1682506622048_0002/io.delta_delta-core_2.12-2.2.0.jar\n",
      "2023-04-26 11:32:43,482 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2023-04-26 11:32:46,045 INFO yarn.Client: Uploading resource file:/home/ubuntu/.ivy2/jars/io.delta_delta-storage-2.2.0.jar -> hdfs://namenode:9000/user/ubuntu/.sparkStaging/application_1682506622048_0002/io.delta_delta-storage-2.2.0.jar\n",
      "2023-04-26 11:32:46,076 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2023-04-26 11:32:46,103 INFO yarn.Client: Uploading resource file:/home/ubuntu/.ivy2/jars/org.antlr_antlr4-runtime-4.8.jar -> hdfs://namenode:9000/user/ubuntu/.sparkStaging/application_1682506622048_0002/org.antlr_antlr4-runtime-4.8.jar\n",
      "2023-04-26 11:32:46,141 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2023-04-26 11:32:46,183 INFO yarn.Client: Uploading resource file:/home/ubuntu/spark-3.3.2-bin-without-hadoop/python/lib/pyspark.zip -> hdfs://namenode:9000/user/ubuntu/.sparkStaging/application_1682506622048_0002/pyspark.zip\n",
      "2023-04-26 11:32:46,243 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2023-04-26 11:32:46,351 INFO yarn.Client: Uploading resource file:/home/ubuntu/spark-3.3.2-bin-without-hadoop/python/lib/py4j-0.10.9.5-src.zip -> hdfs://namenode:9000/user/ubuntu/.sparkStaging/application_1682506622048_0002/py4j-0.10.9.5-src.zip\n",
      "2023-04-26 11:32:46,389 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2023-04-26 11:32:46,456 WARN yarn.Client: Same path resource file:///home/ubuntu/.ivy2/jars/io.delta_delta-core_2.12-2.2.0.jar added multiple times to distributed cache.\n",
      "2023-04-26 11:32:46,458 WARN yarn.Client: Same path resource file:///home/ubuntu/.ivy2/jars/io.delta_delta-storage-2.2.0.jar added multiple times to distributed cache.\n",
      "2023-04-26 11:32:46,458 WARN yarn.Client: Same path resource file:///home/ubuntu/.ivy2/jars/org.antlr_antlr4-runtime-4.8.jar added multiple times to distributed cache.\n",
      "2023-04-26 11:32:47,282 INFO yarn.Client: Uploading resource file:/tmp/spark-9107ce6b-4f7a-4db3-8734-eb2c972b755f/__spark_conf__327497306240443757.zip -> hdfs://namenode:9000/user/ubuntu/.sparkStaging/application_1682506622048_0002/__spark_conf__.zip\n",
      "2023-04-26 11:32:47,460 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2023-04-26 11:32:47,602 INFO spark.SecurityManager: Changing view acls to: ubuntu\n",
      "2023-04-26 11:32:47,603 INFO spark.SecurityManager: Changing modify acls to: ubuntu\n",
      "2023-04-26 11:32:47,603 INFO spark.SecurityManager: Changing view acls groups to: \n",
      "2023-04-26 11:32:47,603 INFO spark.SecurityManager: Changing modify acls groups to: \n",
      "2023-04-26 11:32:47,603 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(ubuntu); groups with view permissions: Set(); users  with modify permissions: Set(ubuntu); groups with modify permissions: Set()\n",
      "2023-04-26 11:32:47,658 INFO yarn.Client: Submitting application application_1682506622048_0002 to ResourceManager\n",
      "2023-04-26 11:32:47,822 INFO impl.YarnClientImpl: Submitted application application_1682506622048_0002\n",
      "2023-04-26 11:32:48,829 INFO yarn.Client: Application report for application_1682506622048_0002 (state: ACCEPTED)\n",
      "2023-04-26 11:32:48,836 INFO yarn.Client: \n",
      "\t client token: N/A\n",
      "\t diagnostics: AM container is launched, waiting for AM container to Register with RM\n",
      "\t ApplicationMaster host: N/A\n",
      "\t ApplicationMaster RPC port: -1\n",
      "\t queue: default\n",
      "\t start time: 1682508767720\n",
      "\t final status: UNDEFINED\n",
      "\t tracking URL: http://namenode:8088/proxy/application_1682506622048_0002/\n",
      "\t user: ubuntu\n",
      "2023-04-26 11:32:49,840 INFO yarn.Client: Application report for application_1682506622048_0002 (state: ACCEPTED)\n",
      "2023-04-26 11:32:50,845 INFO yarn.Client: Application report for application_1682506622048_0002 (state: ACCEPTED)\n",
      "2023-04-26 11:32:51,857 INFO yarn.Client: Application report for application_1682506622048_0002 (state: ACCEPTED)\n",
      "2023-04-26 11:32:52,861 INFO yarn.Client: Application report for application_1682506622048_0002 (state: ACCEPTED)\n",
      "2023-04-26 11:32:53,867 INFO yarn.Client: Application report for application_1682506622048_0002 (state: ACCEPTED)\n",
      "2023-04-26 11:32:54,871 INFO yarn.Client: Application report for application_1682506622048_0002 (state: ACCEPTED)\n",
      "2023-04-26 11:32:55,875 INFO yarn.Client: Application report for application_1682506622048_0002 (state: ACCEPTED)\n",
      "2023-04-26 11:32:56,877 INFO yarn.Client: Application report for application_1682506622048_0002 (state: ACCEPTED)\n",
      "2023-04-26 11:32:57,882 INFO yarn.Client: Application report for application_1682506622048_0002 (state: ACCEPTED)\n",
      "2023-04-26 11:32:58,884 INFO yarn.Client: Application report for application_1682506622048_0002 (state: ACCEPTED)\n",
      "2023-04-26 11:32:59,888 INFO yarn.Client: Application report for application_1682506622048_0002 (state: ACCEPTED)\n",
      "2023-04-26 11:33:00,892 INFO yarn.Client: Application report for application_1682506622048_0002 (state: ACCEPTED)\n",
      "2023-04-26 11:33:01,896 INFO yarn.Client: Application report for application_1682506622048_0002 (state: ACCEPTED)\n",
      "2023-04-26 11:33:02,623 INFO cluster.YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> namenode, PROXY_URI_BASES -> http://namenode:8088/proxy/application_1682506622048_0002), /proxy/application_1682506622048_0002\n",
      "2023-04-26 11:33:02,902 INFO yarn.Client: Application report for application_1682506622048_0002 (state: RUNNING)\n",
      "2023-04-26 11:33:02,903 INFO yarn.Client: \n",
      "\t client token: N/A\n",
      "\t diagnostics: N/A\n",
      "\t ApplicationMaster host: 192.168.1.8\n",
      "\t ApplicationMaster RPC port: -1\n",
      "\t queue: default\n",
      "\t start time: 1682508767720\n",
      "\t final status: UNDEFINED\n",
      "\t tracking URL: http://namenode:8088/proxy/application_1682506622048_0002/\n",
      "\t user: ubuntu\n",
      "2023-04-26 11:33:02,906 INFO cluster.YarnClientSchedulerBackend: Application application_1682506622048_0002 has started running.\n",
      "2023-04-26 11:33:02,929 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45149.\n",
      "2023-04-26 11:33:02,929 INFO netty.NettyBlockTransferService: Server created on namenode:45149\n",
      "2023-04-26 11:33:02,932 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "2023-04-26 11:33:02,946 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, namenode, 45149, None)\n",
      "2023-04-26 11:33:02,953 INFO storage.BlockManagerMasterEndpoint: Registering block manager namenode:45149 with 3.6 GiB RAM, BlockManagerId(driver, namenode, 45149, None)\n",
      "2023-04-26 11:33:02,957 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, namenode, 45149, None)\n",
      "2023-04-26 11:33:02,959 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, namenode, 45149, None)\n",
      "2023-04-26 11:33:03,305 INFO history.SingleEventLogFileWriter: Logging events to hdfs://namenode:9000/spark-logs/application_1682506622048_0002.inprogress\n",
      "2023-04-26 11:33:03,734 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2023-04-26 11:33:03,850 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@12fee641{/,null,STOPPED,@Spark}\n",
      "2023-04-26 11:33:03,853 INFO ui.ServerInfo: Adding filter to /jobs: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-04-26 11:33:03,891 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@820afd{/jobs,null,AVAILABLE,@Spark}\n",
      "2023-04-26 11:33:03,892 INFO ui.ServerInfo: Adding filter to /jobs/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-04-26 11:33:03,894 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2e48f414{/jobs/json,null,AVAILABLE,@Spark}\n",
      "2023-04-26 11:33:03,894 INFO ui.ServerInfo: Adding filter to /jobs/job: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-04-26 11:33:03,896 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1771a497{/jobs/job,null,AVAILABLE,@Spark}\n",
      "2023-04-26 11:33:03,900 INFO ui.ServerInfo: Adding filter to /jobs/job/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-04-26 11:33:03,902 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@72198cf4{/jobs/job/json,null,AVAILABLE,@Spark}\n",
      "2023-04-26 11:33:03,902 INFO ui.ServerInfo: Adding filter to /stages: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-04-26 11:33:03,904 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4919a0ed{/stages,null,AVAILABLE,@Spark}\n",
      "2023-04-26 11:33:03,904 INFO ui.ServerInfo: Adding filter to /stages/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-04-26 11:33:03,906 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2bc3b63a{/stages/json,null,AVAILABLE,@Spark}\n",
      "2023-04-26 11:33:03,914 INFO ui.ServerInfo: Adding filter to /stages/stage: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-04-26 11:33:03,916 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2e5e13f8{/stages/stage,null,AVAILABLE,@Spark}\n",
      "2023-04-26 11:33:03,916 INFO ui.ServerInfo: Adding filter to /stages/stage/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-04-26 11:33:03,920 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2ef2e8d2{/stages/stage/json,null,AVAILABLE,@Spark}\n",
      "2023-04-26 11:33:03,921 INFO ui.ServerInfo: Adding filter to /stages/pool: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-04-26 11:33:03,922 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6f1bb243{/stages/pool,null,AVAILABLE,@Spark}\n",
      "2023-04-26 11:33:03,923 INFO ui.ServerInfo: Adding filter to /stages/pool/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-04-26 11:33:03,927 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@61808078{/stages/pool/json,null,AVAILABLE,@Spark}\n",
      "2023-04-26 11:33:03,927 INFO ui.ServerInfo: Adding filter to /storage: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-04-26 11:33:03,929 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@34a3013b{/storage,null,AVAILABLE,@Spark}\n",
      "2023-04-26 11:33:03,929 INFO ui.ServerInfo: Adding filter to /storage/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-04-26 11:33:03,931 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@eb92a49{/storage/json,null,AVAILABLE,@Spark}\n",
      "2023-04-26 11:33:03,937 INFO ui.ServerInfo: Adding filter to /storage/rdd: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-04-26 11:33:03,939 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6fe35dba{/storage/rdd,null,AVAILABLE,@Spark}\n",
      "2023-04-26 11:33:03,939 INFO ui.ServerInfo: Adding filter to /storage/rdd/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-04-26 11:33:03,943 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2317e83c{/storage/rdd/json,null,AVAILABLE,@Spark}\n",
      "2023-04-26 11:33:03,943 INFO ui.ServerInfo: Adding filter to /environment: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-04-26 11:33:03,944 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@28b86647{/environment,null,AVAILABLE,@Spark}\n",
      "2023-04-26 11:33:03,945 INFO ui.ServerInfo: Adding filter to /environment/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-04-26 11:33:03,948 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@739fa666{/environment/json,null,AVAILABLE,@Spark}\n",
      "2023-04-26 11:33:03,948 INFO ui.ServerInfo: Adding filter to /executors: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-04-26 11:33:03,952 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@384a5acd{/executors,null,AVAILABLE,@Spark}\n",
      "2023-04-26 11:33:03,952 INFO ui.ServerInfo: Adding filter to /executors/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-04-26 11:33:03,953 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@732ada0b{/executors/json,null,AVAILABLE,@Spark}\n",
      "2023-04-26 11:33:03,955 INFO ui.ServerInfo: Adding filter to /executors/threadDump: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-04-26 11:33:03,958 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5acc5b04{/executors/threadDump,null,AVAILABLE,@Spark}\n",
      "2023-04-26 11:33:03,959 INFO ui.ServerInfo: Adding filter to /executors/threadDump/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-04-26 11:33:03,960 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2a4c7817{/executors/threadDump/json,null,AVAILABLE,@Spark}\n",
      "2023-04-26 11:33:03,960 INFO ui.ServerInfo: Adding filter to /static: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-04-26 11:33:04,022 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7f533d9b{/static,null,AVAILABLE,@Spark}\n",
      "2023-04-26 11:33:04,023 INFO ui.ServerInfo: Adding filter to /: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-04-26 11:33:04,025 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@767f9538{/,null,AVAILABLE,@Spark}\n",
      "2023-04-26 11:33:04,026 INFO ui.ServerInfo: Adding filter to /api: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-04-26 11:33:04,030 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2d020988{/api,null,AVAILABLE,@Spark}\n",
      "2023-04-26 11:33:04,032 INFO ui.ServerInfo: Adding filter to /jobs/job/kill: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-04-26 11:33:04,040 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2441914f{/jobs/job/kill,null,AVAILABLE,@Spark}\n",
      "2023-04-26 11:33:04,040 INFO ui.ServerInfo: Adding filter to /stages/stage/kill: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-04-26 11:33:04,042 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7829ee83{/stages/stage/kill,null,AVAILABLE,@Spark}\n",
      "2023-04-26 11:33:04,051 INFO ui.ServerInfo: Adding filter to /metrics/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-04-26 11:33:04,055 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2b7fd86e{/metrics/json,null,AVAILABLE,@Spark}\n",
      "2023-04-26 11:33:04,058 INFO cluster.YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after waiting maxRegisteredResourcesWaitingTime: 30000000000(ns)\n",
      "2023-04-26 11:33:04,514 INFO cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.context import SparkContext\n",
    "\n",
    "import pyspark\n",
    "from delta import *\n",
    "\n",
    "#Create a session suitable for a delta table\n",
    "builder = pyspark.sql.SparkSession.builder.appName(\"MyApp\").master('yarn') \\\n",
    "    .config(\"spark.executor.instances\", 12) \\\n",
    "    .config(\"spark.executor.memory\", \"1536m\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "\n",
    "#3 nodes with each 4 instances so 12 executors in total, one of which is the driver. \n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()\n",
    "\n",
    "# This reduces the amount of logs we recieve, only priting ERROR level or higher.\n",
    "sc = spark.sparkContext\n",
    "logger = sc._jvm.org.apache.log4j\n",
    "logger.LogManager.getLogger(\"org\"). setLevel( logger.Level.ERROR )\n",
    "logger.LogManager.getLogger(\"akka\").setLevel( logger.Level.ERROR )\n",
    "sc.setLogLevel('ERROR')\n",
    "\n",
    "#Set schema and load dataframe\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "#Introduce schema\n",
    "schema = StructType([\n",
    "    StructField(\"ID\", IntegerType(), False),\n",
    "    StructField(\"title\", StringType(), False),\n",
    "    StructField(\"date\", StringType(), False),\n",
    "    StructField(\"categories\", StringType(), True),\n",
    "    StructField(\"subtitles\", StringType(), True),\n",
    "    StructField(\"related\", StringType(), True),\n",
    "    StructField(\"text\", StringType(), False)\n",
    "])\n",
    "\n",
    "#read in our csv file\n",
    "df = spark.read.csv(\"/simplewiki/output_full_2g.csv\", sep = ',', header = True, schema = schema)\n",
    "\n",
    "#fix date column: remove []\n",
    "df = df.withColumn('title', regexp_replace('title', \"[\\[\\]']\", ''))\n",
    "\n",
    "#fix date column: remove [] and set to datetype\n",
    "df = df.withColumn('date', regexp_replace('date', \"[\\[\\]']\", ''))\n",
    "df = df.withColumn('date', to_date('date', 'yyyy-MM-dd\\'T\\'HH:mm:ss\\'Z\\''))\n",
    "\n",
    "df = df.filter(col(\"text\").isNotNull())\\\n",
    "        .filter(col(\"ID\").isNotNull())\\\n",
    "        .filter(col(\"title\").isNotNull())\n",
    "\n",
    "df = df.withColumn(\"text\", lower(regexp_replace(df.text, '[^\\w\\s]|\\t|\\d+', ' ')))\n",
    "\n",
    "#write the df to a unmanaged delta table. \n",
    "df.write.format(\"delta\").mode(\"overwrite\").save(\"hdfs:///table_2g\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keyword extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/ubuntu/spark-3.3.2-bin-without-hadoop/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/ubuntu/.ivy2/cache\n",
      "The jars for the packages stored in: /home/ubuntu/.ivy2/jars\n",
      "io.delta#delta-core_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-e3e5623f-08af-407f-9fa4-2c9bebf19cda;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-core_2.12;2.2.0 in central\n",
      "\tfound io.delta#delta-storage;2.2.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.8 in central\n",
      ":: resolution report :: resolve 478ms :: artifacts dl 24ms\n",
      "\t:: modules in use:\n",
      "\tio.delta#delta-core_2.12;2.2.0 from central in [default]\n",
      "\tio.delta#delta-storage;2.2.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.8 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-e3e5623f-08af-407f-9fa4-2c9bebf19cda\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/9ms)\n",
      "2023-05-03 15:55:05,189 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "2023-05-03 15:55:06,203 INFO spark.SparkContext: Running Spark version 3.3.2\n",
      "2023-05-03 15:55:06,262 INFO resource.ResourceUtils: ==============================================================\n",
      "2023-05-03 15:55:06,263 INFO resource.ResourceUtils: No custom resources configured for spark.driver.\n",
      "2023-05-03 15:55:06,264 INFO resource.ResourceUtils: ==============================================================\n",
      "2023-05-03 15:55:06,265 INFO spark.SparkContext: Submitted application: MyApp\n",
      "2023-05-03 15:55:06,324 INFO resource.ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1536, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
      "2023-05-03 15:55:06,347 INFO resource.ResourceProfile: Limiting resource is cpus at 1 tasks per executor\n",
      "2023-05-03 15:55:06,353 INFO resource.ResourceProfileManager: Added ResourceProfile id: 0\n",
      "2023-05-03 15:55:06,513 INFO spark.SecurityManager: Changing view acls to: ubuntu\n",
      "2023-05-03 15:55:06,514 INFO spark.SecurityManager: Changing modify acls to: ubuntu\n",
      "2023-05-03 15:55:06,516 INFO spark.SecurityManager: Changing view acls groups to: \n",
      "2023-05-03 15:55:06,516 INFO spark.SecurityManager: Changing modify acls groups to: \n",
      "2023-05-03 15:55:06,517 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(ubuntu); groups with view permissions: Set(); users  with modify permissions: Set(ubuntu); groups with modify permissions: Set()\n",
      "2023-05-03 15:55:07,362 INFO util.Utils: Successfully started service 'sparkDriver' on port 38467.\n",
      "2023-05-03 15:55:07,512 INFO spark.SparkEnv: Registering MapOutputTracker\n",
      "2023-05-03 15:55:07,598 INFO spark.SparkEnv: Registering BlockManagerMaster\n",
      "2023-05-03 15:55:07,671 INFO storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "2023-05-03 15:55:07,672 INFO storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "2023-05-03 15:55:07,875 INFO spark.SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "2023-05-03 15:55:08,051 INFO storage.DiskBlockManager: Created local directory at /tmp/blockmgr-d29cd4af-42fa-4705-9a57-22a8f19dc1ca\n",
      "2023-05-03 15:55:08,092 INFO memory.MemoryStore: MemoryStore started with capacity 3.6 GiB\n",
      "2023-05-03 15:55:08,185 INFO spark.SparkEnv: Registering OutputCommitCoordinator\n",
      "2023-05-03 15:55:08,361 INFO util.log: Logging initialized @11634ms to org.sparkproject.jetty.util.log.Slf4jLog\n",
      "2023-05-03 15:55:08,696 INFO server.Server: jetty-9.4.48.v20220622; built: 2022-06-21T20:42:25.880Z; git: 6b67c5719d1f4371b33655ff2d047d24e171e49a; jvm 1.8.0_362-8u362-ga-0ubuntu1~20.04.1-b09\n",
      "2023-05-03 15:55:08,803 INFO server.Server: Started @12078ms\n",
      "2023-05-03 15:55:08,900 INFO server.AbstractConnector: Started ServerConnector@3e40163a{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}\n",
      "2023-05-03 15:55:08,901 INFO util.Utils: Successfully started service 'SparkUI' on port 4040.\n",
      "2023-05-03 15:55:08,982 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6a6475c1{/,null,AVAILABLE,@Spark}\n",
      "2023-05-03 15:55:10,454 INFO client.RMProxy: Connecting to ResourceManager at namenode/192.168.1.93:8032\n",
      "2023-05-03 15:55:12,534 INFO conf.Configuration: resource-types.xml not found\n",
      "2023-05-03 15:55:12,535 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2023-05-03 15:55:12,574 INFO yarn.Client: Verifying our application has not requested more than the maximum memory capability of the cluster (8192 MB per container)\n",
      "2023-05-03 15:55:12,576 INFO yarn.Client: Will allocate AM container, with 896 MB memory including 384 MB overhead\n",
      "2023-05-03 15:55:12,577 INFO yarn.Client: Setting up container launch context for our AM\n",
      "2023-05-03 15:55:12,590 INFO yarn.Client: Setting up the launch environment for our AM container\n",
      "2023-05-03 15:55:12,600 INFO yarn.Client: Preparing resources for our AM container\n",
      "2023-05-03 15:55:12,669 WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n",
      "2023-05-03 15:55:19,869 INFO yarn.Client: Uploading resource file:/tmp/spark-86a1f6fb-6b39-4e3b-a8c5-48c61f1a17ee/__spark_libs__4351246804354956558.zip -> hdfs://namenode:9000/user/ubuntu/.sparkStaging/application_1682665359283_0032/__spark_libs__4351246804354956558.zip\n",
      "2023-05-03 15:55:20,661 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2023-05-03 15:55:21,780 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2023-05-03 15:55:22,343 INFO yarn.Client: Uploading resource file:/home/ubuntu/.ivy2/jars/io.delta_delta-core_2.12-2.2.0.jar -> hdfs://namenode:9000/user/ubuntu/.sparkStaging/application_1682665359283_0032/io.delta_delta-core_2.12-2.2.0.jar\n",
      "2023-05-03 15:55:22,408 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2023-05-03 15:55:22,490 INFO yarn.Client: Uploading resource file:/home/ubuntu/.ivy2/jars/io.delta_delta-storage-2.2.0.jar -> hdfs://namenode:9000/user/ubuntu/.sparkStaging/application_1682665359283_0032/io.delta_delta-storage-2.2.0.jar\n",
      "2023-05-03 15:55:22,528 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2023-05-03 15:55:22,592 INFO yarn.Client: Uploading resource file:/home/ubuntu/.ivy2/jars/org.antlr_antlr4-runtime-4.8.jar -> hdfs://namenode:9000/user/ubuntu/.sparkStaging/application_1682665359283_0032/org.antlr_antlr4-runtime-4.8.jar\n",
      "2023-05-03 15:55:22,648 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2023-05-03 15:55:22,724 INFO yarn.Client: Uploading resource file:/home/ubuntu/spark-3.3.2-bin-without-hadoop/python/lib/pyspark.zip -> hdfs://namenode:9000/user/ubuntu/.sparkStaging/application_1682665359283_0032/pyspark.zip\n",
      "2023-05-03 15:55:22,757 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2023-05-03 15:55:22,859 INFO yarn.Client: Uploading resource file:/home/ubuntu/spark-3.3.2-bin-without-hadoop/python/lib/py4j-0.10.9.5-src.zip -> hdfs://namenode:9000/user/ubuntu/.sparkStaging/application_1682665359283_0032/py4j-0.10.9.5-src.zip\n",
      "2023-05-03 15:55:22,923 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2023-05-03 15:55:22,987 WARN yarn.Client: Same path resource file:///home/ubuntu/.ivy2/jars/io.delta_delta-core_2.12-2.2.0.jar added multiple times to distributed cache.\n",
      "2023-05-03 15:55:22,988 WARN yarn.Client: Same path resource file:///home/ubuntu/.ivy2/jars/io.delta_delta-storage-2.2.0.jar added multiple times to distributed cache.\n",
      "2023-05-03 15:55:22,988 WARN yarn.Client: Same path resource file:///home/ubuntu/.ivy2/jars/org.antlr_antlr4-runtime-4.8.jar added multiple times to distributed cache.\n",
      "2023-05-03 15:55:23,664 INFO yarn.Client: Uploading resource file:/tmp/spark-86a1f6fb-6b39-4e3b-a8c5-48c61f1a17ee/__spark_conf__1107322387278422188.zip -> hdfs://namenode:9000/user/ubuntu/.sparkStaging/application_1682665359283_0032/__spark_conf__.zip\n",
      "2023-05-03 15:55:23,698 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2023-05-03 15:55:24,202 INFO spark.SecurityManager: Changing view acls to: ubuntu\n",
      "2023-05-03 15:55:24,203 INFO spark.SecurityManager: Changing modify acls to: ubuntu\n",
      "2023-05-03 15:55:24,203 INFO spark.SecurityManager: Changing view acls groups to: \n",
      "2023-05-03 15:55:24,203 INFO spark.SecurityManager: Changing modify acls groups to: \n",
      "2023-05-03 15:55:24,204 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(ubuntu); groups with view permissions: Set(); users  with modify permissions: Set(ubuntu); groups with modify permissions: Set()\n",
      "2023-05-03 15:55:24,266 INFO yarn.Client: Submitting application application_1682665359283_0032 to ResourceManager\n",
      "2023-05-03 15:55:24,421 INFO impl.YarnClientImpl: Submitted application application_1682665359283_0032\n",
      "2023-05-03 15:55:25,426 INFO yarn.Client: Application report for application_1682665359283_0032 (state: ACCEPTED)\n",
      "2023-05-03 15:55:25,434 INFO yarn.Client: \n",
      "\t client token: N/A\n",
      "\t diagnostics: AM container is launched, waiting for AM container to Register with RM\n",
      "\t ApplicationMaster host: N/A\n",
      "\t ApplicationMaster RPC port: -1\n",
      "\t queue: default\n",
      "\t start time: 1683129324338\n",
      "\t final status: UNDEFINED\n",
      "\t tracking URL: http://namenode:8088/proxy/application_1682665359283_0032/\n",
      "\t user: ubuntu\n",
      "2023-05-03 15:55:26,436 INFO yarn.Client: Application report for application_1682665359283_0032 (state: ACCEPTED)\n",
      "2023-05-03 15:55:27,439 INFO yarn.Client: Application report for application_1682665359283_0032 (state: ACCEPTED)\n",
      "2023-05-03 15:55:28,441 INFO yarn.Client: Application report for application_1682665359283_0032 (state: ACCEPTED)\n",
      "2023-05-03 15:55:29,444 INFO yarn.Client: Application report for application_1682665359283_0032 (state: ACCEPTED)\n",
      "2023-05-03 15:55:30,447 INFO yarn.Client: Application report for application_1682665359283_0032 (state: ACCEPTED)\n",
      "2023-05-03 15:55:31,246 INFO cluster.YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> namenode, PROXY_URI_BASES -> http://namenode:8088/proxy/application_1682665359283_0032), /proxy/application_1682665359283_0032\n",
      "2023-05-03 15:55:31,450 INFO yarn.Client: Application report for application_1682665359283_0032 (state: RUNNING)\n",
      "2023-05-03 15:55:31,451 INFO yarn.Client: \n",
      "\t client token: N/A\n",
      "\t diagnostics: N/A\n",
      "\t ApplicationMaster host: 192.168.1.145\n",
      "\t ApplicationMaster RPC port: -1\n",
      "\t queue: default\n",
      "\t start time: 1683129324338\n",
      "\t final status: UNDEFINED\n",
      "\t tracking URL: http://namenode:8088/proxy/application_1682665359283_0032/\n",
      "\t user: ubuntu\n",
      "2023-05-03 15:55:31,454 INFO cluster.YarnClientSchedulerBackend: Application application_1682665359283_0032 has started running.\n",
      "2023-05-03 15:55:31,488 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 41999.\n",
      "2023-05-03 15:55:31,488 INFO netty.NettyBlockTransferService: Server created on namenode:41999\n",
      "2023-05-03 15:55:31,492 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "2023-05-03 15:55:31,504 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, namenode, 41999, None)\n",
      "2023-05-03 15:55:31,510 INFO storage.BlockManagerMasterEndpoint: Registering block manager namenode:41999 with 3.6 GiB RAM, BlockManagerId(driver, namenode, 41999, None)\n",
      "2023-05-03 15:55:31,519 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, namenode, 41999, None)\n",
      "2023-05-03 15:55:31,523 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, namenode, 41999, None)\n",
      "2023-05-03 15:55:31,849 INFO history.SingleEventLogFileWriter: Logging events to hdfs://namenode:9000/spark-logs/application_1682665359283_0032.inprogress\n",
      "2023-05-03 15:55:32,165 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2023-05-03 15:55:32,272 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@6a6475c1{/,null,STOPPED,@Spark}\n",
      "2023-05-03 15:55:32,274 INFO ui.ServerInfo: Adding filter to /jobs: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-05-03 15:55:32,285 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@9d222ba{/jobs,null,AVAILABLE,@Spark}\n",
      "2023-05-03 15:55:32,286 INFO ui.ServerInfo: Adding filter to /jobs/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-05-03 15:55:32,289 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@409aa3c6{/jobs/json,null,AVAILABLE,@Spark}\n",
      "2023-05-03 15:55:32,290 INFO ui.ServerInfo: Adding filter to /jobs/job: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-05-03 15:55:32,292 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6384c985{/jobs/job,null,AVAILABLE,@Spark}\n",
      "2023-05-03 15:55:32,294 INFO ui.ServerInfo: Adding filter to /jobs/job/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-05-03 15:55:32,296 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@41613629{/jobs/job/json,null,AVAILABLE,@Spark}\n",
      "2023-05-03 15:55:32,296 INFO ui.ServerInfo: Adding filter to /stages: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-05-03 15:55:32,303 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@8707759{/stages,null,AVAILABLE,@Spark}\n",
      "2023-05-03 15:55:32,304 INFO ui.ServerInfo: Adding filter to /stages/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-05-03 15:55:32,307 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@541569f0{/stages/json,null,AVAILABLE,@Spark}\n",
      "2023-05-03 15:55:32,308 INFO ui.ServerInfo: Adding filter to /stages/stage: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-05-03 15:55:32,316 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@21a084b5{/stages/stage,null,AVAILABLE,@Spark}\n",
      "2023-05-03 15:55:32,317 INFO ui.ServerInfo: Adding filter to /stages/stage/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-05-03 15:55:32,320 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@79f3af74{/stages/stage/json,null,AVAILABLE,@Spark}\n",
      "2023-05-03 15:55:32,326 INFO ui.ServerInfo: Adding filter to /stages/pool: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-05-03 15:55:32,338 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@34bd486e{/stages/pool,null,AVAILABLE,@Spark}\n",
      "2023-05-03 15:55:32,339 INFO ui.ServerInfo: Adding filter to /stages/pool/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-05-03 15:55:32,341 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7454fe5c{/stages/pool/json,null,AVAILABLE,@Spark}\n",
      "2023-05-03 15:55:32,349 INFO ui.ServerInfo: Adding filter to /storage: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-05-03 15:55:32,355 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4feef6fa{/storage,null,AVAILABLE,@Spark}\n",
      "2023-05-03 15:55:32,356 INFO ui.ServerInfo: Adding filter to /storage/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-05-03 15:55:32,358 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@55829055{/storage/json,null,AVAILABLE,@Spark}\n",
      "2023-05-03 15:55:32,362 INFO ui.ServerInfo: Adding filter to /storage/rdd: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-05-03 15:55:32,375 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3dd1d29d{/storage/rdd,null,AVAILABLE,@Spark}\n",
      "2023-05-03 15:55:32,377 INFO ui.ServerInfo: Adding filter to /storage/rdd/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-05-03 15:55:32,379 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@976b68{/storage/rdd/json,null,AVAILABLE,@Spark}\n",
      "2023-05-03 15:55:32,383 INFO ui.ServerInfo: Adding filter to /environment: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-05-03 15:55:32,401 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@628aff3{/environment,null,AVAILABLE,@Spark}\n",
      "2023-05-03 15:55:32,402 INFO ui.ServerInfo: Adding filter to /environment/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-05-03 15:55:32,404 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4c23cb18{/environment/json,null,AVAILABLE,@Spark}\n",
      "2023-05-03 15:55:32,404 INFO ui.ServerInfo: Adding filter to /executors: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-05-03 15:55:32,405 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5575ce45{/executors,null,AVAILABLE,@Spark}\n",
      "2023-05-03 15:55:32,405 INFO ui.ServerInfo: Adding filter to /executors/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-05-03 15:55:32,432 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1bf0672c{/executors/json,null,AVAILABLE,@Spark}\n",
      "2023-05-03 15:55:32,432 INFO ui.ServerInfo: Adding filter to /executors/threadDump: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-05-03 15:55:32,439 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6c29ef3c{/executors/threadDump,null,AVAILABLE,@Spark}\n",
      "2023-05-03 15:55:32,442 INFO ui.ServerInfo: Adding filter to /executors/threadDump/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-05-03 15:55:32,448 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5ad34270{/executors/threadDump/json,null,AVAILABLE,@Spark}\n",
      "2023-05-03 15:55:32,449 INFO ui.ServerInfo: Adding filter to /static: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-05-03 15:55:32,533 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@40bd9cf7{/static,null,AVAILABLE,@Spark}\n",
      "2023-05-03 15:55:32,534 INFO ui.ServerInfo: Adding filter to /: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-05-03 15:55:32,535 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@65b93c8{/,null,AVAILABLE,@Spark}\n",
      "2023-05-03 15:55:32,536 INFO ui.ServerInfo: Adding filter to /api: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-05-03 15:55:32,542 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7b8db722{/api,null,AVAILABLE,@Spark}\n",
      "2023-05-03 15:55:32,543 INFO ui.ServerInfo: Adding filter to /jobs/job/kill: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-05-03 15:55:32,545 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6a2b25d{/jobs/job/kill,null,AVAILABLE,@Spark}\n",
      "2023-05-03 15:55:32,545 INFO ui.ServerInfo: Adding filter to /stages/stage/kill: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-05-03 15:55:32,547 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6b84bab{/stages/stage/kill,null,AVAILABLE,@Spark}\n",
      "2023-05-03 15:55:32,561 INFO ui.ServerInfo: Adding filter to /metrics/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2023-05-03 15:55:32,563 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@143f8f45{/metrics/json,null,AVAILABLE,@Spark}\n",
      "2023-05-03 15:55:33,867 INFO cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)\n",
      "2023-05-03 15:55:39,251 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.1.145:49608) with ID 1,  ResourceProfileId 0\n",
      "2023-05-03 15:55:39,499 INFO storage.BlockManagerMasterEndpoint: Registering block manager datanode3:34105 with 639.3 MiB RAM, BlockManagerId(1, datanode3, 34105, None)\n",
      "2023-05-03 15:55:40,091 INFO cluster.YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after waiting maxRegisteredResourcesWaitingTime: 30000000000(ns)\n",
      "2023-05-03 15:55:40,242 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.1.145:53682) with ID 4,  ResourceProfileId 0\n",
      "2023-05-03 15:55:40,520 INFO storage.BlockManagerMasterEndpoint: Registering block manager datanode3:40343 with 639.3 MiB RAM, BlockManagerId(4, datanode3, 40343, None)\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean overlap: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 30:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------------------------------------------------------------------------------------------------------+-------+\n",
      "|ID        |keywords                                                                                                       |overlap|\n",
      "+----------+---------------------------------------------------------------------------------------------------------------+-------+\n",
      "|1146534968|[waleswales, identified, inhabited, humans, years, evidenced, discovery, neanderthal, bontnewydd, palaeolithic]|0      |\n",
      "+----------+---------------------------------------------------------------------------------------------------------------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#start context and session\n",
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.context import SparkContext\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from delta import *\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "builder = pyspark.sql.SparkSession.builder.appName(\"MyApp\").master('yarn') \\\n",
    "    .config(\"spark.executor.instances\", 12) \\\n",
    "    .config(\"spark.executor.memory\", \"1536m\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()\n",
    "\n",
    "# This reduces the amount of logs we recieve, only priting ERROR level or higher.\n",
    "sc = spark.sparkContext\n",
    "logger = sc._jvm.org.apache.log4j\n",
    "logger.LogManager.getLogger(\"org\"). setLevel( logger.Level.ERROR )\n",
    "logger.LogManager.getLogger(\"akka\").setLevel( logger.Level.ERROR )\n",
    "sc.setLogLevel('ERROR')\n",
    "\n",
    "#Read in data\n",
    "dftest = spark.read.format(\"delta\").load(\"hdfs:///table_2g\").limit(2)  # query table by path\n",
    "\n",
    "# convert text and title to an array\n",
    "Ctext1 = dftest.withColumn(\"filt1\", split(\"text\", \" \"))\\\n",
    "               .withColumn(\"Ctit\", split(lower(regexp_replace(dftest.title, '[^\\w\\s]', '')), \" \"))\n",
    "\n",
    "#filter words in title from the array in filt\n",
    "Ctext2 = Ctext1.withColumn(\"filt2\", expr(\"array_except(filt1, Ctit)\"))\n",
    "\n",
    "#Remove empty strings\n",
    "Ctext3 = Ctext2.withColumn(\"filt3\", array_remove(\"filt2\", \"\"))\n",
    "\n",
    "#remove stopwords\n",
    "from pyspark.ml.feature import StopWordsRemover # to remove stop words\n",
    "stopwords_remover = StopWordsRemover(inputCol=\"filt3\", outputCol=\"filtered_map\")\n",
    "Ctext4= stopwords_remover.transform(Ctext3)\n",
    "\n",
    "#TF-IDF = TF* IDF\n",
    "# TF = total number occurences in article / total number of words in article\n",
    "# IDF = log(number of articles / number of articles that contain the word)\n",
    "\n",
    "#Count number of documents to calculate the IDF later\n",
    "num_doc = Ctext4.count()\n",
    "\n",
    "#Count number of words in text\n",
    "Ctext5 = Ctext4.withColumn(\"text_len\", size(Ctext4.filtered_map))\n",
    "\n",
    "#create row for each word in text\n",
    "df_ex= Ctext5.select(\"ID\", \"text_len\" , explode(Ctext5.filtered_map).alias(\"word_split\"))\n",
    "\n",
    "#group the df by the word_split column and count the distinct number of IDs for each word\n",
    "df_count = df_ex.groupBy(\"word_split\").agg(countDistinct(\"ID\").alias(\"num_articles\"))\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "#group the rows for the same word and article (ID) and calculate TF\n",
    "df_ex = df_ex.groupBy(\"ID\", \"word_split\", \"text_len\") \\\n",
    "        .agg(count(\"word_split\").alias(\"count\"), \\\n",
    "             expr(\"count(word_split) / text_len\").alias(\"TF\"))\n",
    "\n",
    "# calculate the idf for every word \n",
    "df_join = df_ex.join(df_count, on=\"word_split\", how=\"left\") #join1\n",
    "\n",
    "df_join = df_join.withColumn(\"IDF\", log(num_doc / df_join.num_articles))\n",
    "\n",
    "#calculate the tf*IDF for every word\n",
    "df_join = df_join.withColumn(\"TF_IDF\", expr(\"TF * IDF\"))\n",
    "\n",
    "#select the 10 keywords\n",
    "from pyspark.sql.window import Window\n",
    "key_part = Window.partitionBy(\"ID\").orderBy(col(\"TF_IDF\").desc())\n",
    "df_key = df_join.withColumn(\"row\",row_number().over(key_part))\\\n",
    "                .filter(col(\"row\") <= 10)\\\n",
    "                .drop(\"row\") \\\n",
    "                .groupby('ID')\\\n",
    "                .agg(collect_list('word_split').alias(\"keywords\"))\n",
    "\n",
    "df_new = dftest.select(\"ID\", \"subtitles\", \"related\", \"categories\")\\\n",
    "                .filter(col(\"subtitles\").isNotNull())\\\n",
    "                .join(df_key, on=\"ID\", how=\"left\")\n",
    "\n",
    "df_new1 = df_new.select(\"ID\", \"keywords\", concat_ws(\" \", col(\"related\"), col(\"subtitles\"), col(\"categories\")).alias(\"con_col\"))\n",
    "\n",
    "df_new1 = df_new1.filter((df_new1.keywords.isNotNull()) & (df_new1.con_col.isNotNull()))\n",
    "\n",
    "df_new2 = df_new1.select(\"ID\", \"keywords\", lower(\"con_col\").alias(\"con_col_L\"))\n",
    "\n",
    "# #transform to list and make it distinct\n",
    "df_new3 = df_new2.select(\"ID\", \"keywords\", array_distinct(split(\"con_col_L\", \" \")).alias(\"con_split\"))\n",
    "\n",
    "# #compute the overlap\n",
    "df_new4 = df_new3.select(\"ID\", \"keywords\", size(array_intersect(\"con_split\", \"keywords\")).alias(\"overlap\"))\n",
    "\n",
    "# # Compute the mean of the overlap column\n",
    "mean_overlap = df_new4.agg(mean(\"overlap\")).collect()[0][0]\n",
    "\n",
    "# Print the mean value\n",
    "print(\"Mean overlap:\", mean_overlap)\n",
    "\n",
    "df_new4.show(truncate=False)\n",
    "\n",
    "#write the result to a new deltatable\n",
    "# df_new.write.format(\"delta\").mode(\"append\").save(\"hdfs:///table_2g_demo\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reduced Dataframe sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean overlap: 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 43:=>(18 + 1) / 19][Stage 44:=>(17 + 2) / 19][Stage 69:>   (0 + 1) / 1]9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------------------------------------------------------------------------------------------------------+-------+\n",
      "|ID        |keywords                                                                                                       |overlap|\n",
      "+----------+---------------------------------------------------------------------------------------------------------------+-------+\n",
      "|1133240144|[zakraj, ek, july, ndash, september, slovene, mathematician, computer, scientist, born]                        |1      |\n",
      "|1146534968|[waleswales, identified, inhabited, humans, years, evidenced, discovery, neanderthal, bontnewydd, palaeolithic]|0      |\n",
      "+----------+---------------------------------------------------------------------------------------------------------------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 43:===============>(18 + 1) / 19][Stage 44:===============>(18 + 1) / 19]\r"
     ]
    }
   ],
   "source": [
    "#start context and session\n",
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.context import SparkContext\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from delta import *\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "builder = pyspark.sql.SparkSession.builder.appName(\"MyApp\").master('yarn') \\\n",
    "        .config(\"spark.executor.instances\", 12) \\\n",
    "        .config(\"spark.executor.memory\", \"1536m\") \\\n",
    "        .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "        .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "\n",
    "######fuck autoBroadcastJoinThreshold disabled!!!!!!!!!!!!\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()\n",
    "\n",
    "# This reduces the amount of logs we recieve, only priting ERROR level or higher.\n",
    "sc = spark.sparkContext\n",
    "logger = sc._jvm.org.apache.log4j\n",
    "logger.LogManager.getLogger(\"org\"). setLevel( logger.Level.ERROR )\n",
    "logger.LogManager.getLogger(\"akka\").setLevel( logger.Level.ERROR )\n",
    "sc.setLogLevel('ERROR')\n",
    "\n",
    "#Read in df from deltatable\n",
    "dftest = spark.read.format(\"delta\").load(\"hdfs:///table_2g\").limit(2) # query table by path\n",
    "\n",
    "# convert title and text to an array\n",
    "Ctit = split(lower(regexp_replace(dftest.title, '[^\\w\\s]', '')), \" \")\n",
    "\n",
    "Ctext1 = dftest.select(\"ID\", split(\"text\", \" \").alias(\"filt1\"), Ctit.alias(\"Ctit\"))\n",
    "\n",
    "#filter words in title from the array in filt\n",
    "Ctext2 = Ctext1.select(\"ID\", expr(\"array_except(filt1, Ctit)\").alias(\"filt2\"))\n",
    "\n",
    "#Remove empty strings\n",
    "Ctext3 = Ctext2.select(\"ID\", array_remove(\"filt2\", \"\").alias(\"filt3\"))\n",
    "\n",
    "#Remove stopwords \n",
    "from pyspark.ml.feature import StopWordsRemover # to remove stop words\n",
    "stopwords_remover = StopWordsRemover(inputCol=\"filt3\", outputCol=\"filt4\")\n",
    "Ctext4 = stopwords_remover.transform(Ctext3.select(\"ID\",\"filt3\"))\n",
    "\n",
    "#Count number of documents\n",
    "num_doc = Ctext4.count()\n",
    "\n",
    "#Count number of words in text of each article and create an extra column\n",
    "df_tf1 = Ctext4.select(\"ID\", \"filt4\", size(\"filt4\").alias(\"text_len\"))\n",
    "\n",
    "#Create row for each word in text\n",
    "df_ex = df_tf1.select(\"ID\",\"text_len\", explode(\"filt4\").alias(\"word_split\"))\n",
    "\n",
    "#Find the IDFs for each word\n",
    "import math\n",
    "df_count = df_ex.groupBy(\"word_split\")\\\n",
    "        .agg(log(num_doc/countDistinct(\"ID\")).alias(\"IDF\"))\\\n",
    "        .where(col(\"IDF\") > math.log(num_doc/(num_doc*0.2)))        #filter the words that occur in more than 20% of the documents\n",
    "\n",
    "#Find TF and calculate the TF-IDF\n",
    "df_ex2 = df_ex.groupBy(\"ID\", \"word_split\", \"text_len\") \\\n",
    "        .agg((count(\"word_split\") / col(\"text_len\")).alias(\"TF\"))\n",
    "\n",
    "#Find the IDF for every word using a join and calculate the TF-IDF\n",
    "df_join = df_ex2.join(df_count, on=\"word_split\", how=\"left\")\\\n",
    "                  .select(\"ID\", \"word_split\", expr(\"TF*IDF\").alias(\"TF_IDF\"))\n",
    "\n",
    "#Sort the df by ID\n",
    "from pyspark.sql.window import Window\n",
    "key_part = Window.partitionBy(\"ID\").orderBy(col(\"TF_IDF\").desc())\n",
    "#select the 5 keywords with the highest TF-IDF value \n",
    "df_key = df_join.withColumn(\"row\",row_number().over(key_part))\\\n",
    "                .filter(col(\"row\") <= 10)\\\n",
    "                .drop(\"row\") \\\n",
    "                .groupby('ID')\\\n",
    "                .agg(collect_list('word_split').alias(\"keywords\"))\n",
    "\n",
    "df_new = dftest.select(\"ID\", \"subtitles\", \"related\", \"categories\")\\\n",
    "                .join(df_key.filter(col(\"keywords\").isNotNull()), on=\"ID\", how=\"left\")\n",
    "\n",
    "df_new1 = df_new.select(\"ID\", \"keywords\", concat_ws(\" \", col(\"related\"), col(\"subtitles\"), col(\"categories\")).alias(\"con_col\"))\n",
    "\n",
    "df_new1 = df_new1.filter((df_new1.keywords.isNotNull()) & (df_new1.con_col.isNotNull()))\n",
    "\n",
    "df_new2 = df_new1.select(\"ID\", \"keywords\", lower(\"con_col\").alias(\"con_col_L\"))\n",
    "\n",
    "# #transform to list and make it distinct\n",
    "df_new3 = df_new2.select(\"ID\", \"keywords\", array_distinct(split(\"con_col_L\", \" \")).alias(\"con_split\"))\n",
    "\n",
    "# #compute the overlap\n",
    "df_new4 = df_new3.select(\"ID\", \"keywords\", size(array_intersect(\"con_split\", \"keywords\")).alias(\"overlap\"))\n",
    "\n",
    "# # Compute the mean of the overlap column\n",
    "mean_overlap = df_new4.agg(mean(\"overlap\")).collect()[0][0]\n",
    "\n",
    "# Print the mean value\n",
    "print(\"Mean overlap:\", mean_overlap)\n",
    "\n",
    "df_new4.show(truncate=False)\n",
    "\n",
    "#write the result to a new deltatable\n",
    "# df_new.write.format(\"delta\").mode(\"append\").save(\"hdfs:///table_2g_demo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#addtional analysis\n",
    "df_new3.explain()\n",
    "df_new3.rdd.getNumPartitions()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IO- optimzation: Partitioning (36) with cacheing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-03 16:02:43,021 ERROR cluster.YarnScheduler: Lost executor 6 on datanode2: Container from a bad node: container_1682665359283_0032_01_000007 on host: datanode2. Exit status: 137. Diagnostics: [2023-05-03 16:02:40.033]Container killed on request. Exit code is 137\n",
      "[2023-05-03 16:02:40.034]Container exited with a non-zero exit code 137. \n",
      "[2023-05-03 16:02:40.035]Killed by external signal\n",
      ".\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean overlap: 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 44:===============>(18 + 2) / 19][Stage 73:>                 (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------------------------------------------------------------------------------------------------------+-------+\n",
      "|ID        |keywords                                                                                                       |overlap|\n",
      "+----------+---------------------------------------------------------------------------------------------------------------+-------+\n",
      "|1133240144|[zakraj, ek, july, ndash, september, slovene, mathematician, computer, scientist, born]                        |1      |\n",
      "|1146534968|[waleswales, identified, inhabited, humans, years, evidenced, discovery, neanderthal, bontnewydd, palaeolithic]|0      |\n",
      "+----------+---------------------------------------------------------------------------------------------------------------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 44:===============>(18 + 2) / 19][Stage 73:>                 (0 + 2) / 2]\r"
     ]
    }
   ],
   "source": [
    "#start context and session\n",
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.context import SparkContext\n",
    "\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from delta import *\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "builder = pyspark.sql.SparkSession.builder.appName(\"MyApp\").master('yarn') \\\n",
    "    .config(\"spark.executor.instances\", 12) \\\n",
    "    .config(\"spark.executor.memory\", \"1536m\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()\n",
    "\n",
    "# This reduces the amount of logs we recieve, only priting ERROR level or higher.\n",
    "sc = spark.sparkContext\n",
    "logger = sc._jvm.org.apache.log4j\n",
    "logger.LogManager.getLogger(\"org\"). setLevel( logger.Level.ERROR )\n",
    "logger.LogManager.getLogger(\"akka\").setLevel( logger.Level.ERROR )\n",
    "sc.setLogLevel('ERROR')\n",
    "\n",
    "#Read in df from deltatable\n",
    "##############\n",
    "#optimization#\n",
    "##############\n",
    "\n",
    "dftest = spark.read.format(\"delta\")\\\n",
    "         .option(\"numPartitions\", 72) \\\n",
    "         .option(\"partitionBy\", \"ID\") \\\n",
    "         .load(\"hdfs:///table_2g\")\\\n",
    "         .limit(2)\n",
    "           # query table by path\n",
    "\n",
    "# convert title and text to an array\n",
    "Ctit = split(lower(regexp_replace(dftest.title, '[^\\w\\s]', '')), \" \")\n",
    "\n",
    "Ctext1 = dftest.select(\"ID\", split(\"text\", \" \").alias(\"filt1\"), Ctit.alias(\"Ctit\"))\n",
    "\n",
    "#filter words in title from the array in filt\n",
    "Ctext2 = Ctext1.select(\"ID\", expr(\"array_except(filt1, Ctit)\").alias(\"filt2\"))\n",
    "\n",
    "#Remove empty strings\n",
    "Ctext3 = Ctext2.select(\"ID\", array_remove(\"filt2\", \"\").alias(\"filt3\"))\n",
    "\n",
    "#Remove stopwords \n",
    "from pyspark.ml.feature import StopWordsRemover # to remove stop words\n",
    "stopwords_remover = StopWordsRemover(inputCol=\"filt3\", outputCol=\"filt4\")\n",
    "Ctext4 = stopwords_remover.transform(Ctext3.select(\"ID\",\"filt3\"))\n",
    "\n",
    "#Count number of documents\n",
    "num_doc = Ctext4.count()\n",
    "\n",
    "#Count number of words in text of each article and create an extra column\n",
    "df_tf1 = Ctext4.select(\"ID\", \"filt4\", size(\"filt4\").alias(\"text_len\"))\n",
    "\n",
    "#Create row for each word in text\n",
    "df_ex = df_tf1.select(\"ID\",\"text_len\", explode(\"filt4\").alias(\"word_split\"))\n",
    "\n",
    "#Find the IDFs for each word\n",
    "import math\n",
    "df_count = df_ex.groupBy(\"word_split\")\\\n",
    "        .agg(log(num_doc/countDistinct(\"ID\")).alias(\"IDF\"))\\\n",
    "        .where(col(\"IDF\") > math.log(num_doc/(num_doc*0.2)))        #filter the words that occur in more than 20% of the documents\n",
    "\n",
    "#Find TF and calculate the TF-IDF\n",
    "df_ex2 = df_ex.groupBy(\"ID\", \"word_split\", \"text_len\") \\\n",
    "        .agg((count(\"word_split\") / col(\"text_len\")).alias(\"TF\"))\n",
    "\n",
    "##############\n",
    "#optimization#\n",
    "##############\n",
    "df_ex2.repartition(72, \"word_split\")\\\n",
    "      .cache()\n",
    "\n",
    "#Find the IDF for every word using a join and calculate the TF-IDF\n",
    "df_join = df_ex2.join(df_count, on=\"word_split\", how=\"left\")\\\n",
    "                  .select(\"ID\", \"word_split\", expr(\"TF*IDF\").alias(\"TF_IDF\"))\n",
    "\n",
    "##############\n",
    "#optimization#\n",
    "##############\n",
    "# remove df_ex from cache\n",
    "df_ex2.unpersist()\n",
    "\n",
    "#Sort the df by ID\n",
    "from pyspark.sql.window import Window\n",
    "key_part = Window.partitionBy(\"ID\").orderBy(col(\"TF_IDF\").desc())\n",
    "#select the 5 keywords with the highest TF-IDF value \n",
    "df_key = df_join.withColumn(\"row\",row_number().over(key_part))\\\n",
    "                .filter(col(\"row\") <= 10)\\\n",
    "                .drop(\"row\") \\\n",
    "                .groupby('ID')\\\n",
    "                .agg(collect_list('word_split').alias(\"keywords\"))\n",
    "\n",
    "df_new = dftest.select(\"ID\", \"subtitles\", \"related\", \"categories\")\\\n",
    "                .join(df_key.filter(col(\"keywords\").isNotNull()), on=\"ID\", how=\"left\")\n",
    "\n",
    "df_new1 = df_new.select(\"ID\", \"keywords\", concat_ws(\" \", col(\"related\"), col(\"subtitles\"), col(\"categories\")).alias(\"con_col\"))\n",
    "\n",
    "df_new1 = df_new1.filter((df_new1.keywords.isNotNull()) & (df_new1.con_col.isNotNull()))\n",
    "\n",
    "df_new2 = df_new1.select(\"ID\", \"keywords\", lower(\"con_col\").alias(\"con_col_L\"))\n",
    "\n",
    "# #transform to list and make it distinct\n",
    "df_new3 = df_new2.select(\"ID\", \"keywords\", array_distinct(split(\"con_col_L\", \" \")).alias(\"con_split\"))\n",
    "\n",
    "# #compute the overlap\n",
    "df_new4 = df_new3.select(\"ID\", \"keywords\", size(array_intersect(\"con_split\", \"keywords\")).alias(\"overlap\"))\n",
    "\n",
    "# # Compute the mean of the overlap column\n",
    "mean_overlap = df_new4.agg(mean(\"overlap\")).collect()[0][0]\n",
    "\n",
    "# Print the mean value\n",
    "print(\"Mean overlap:\", mean_overlap)\n",
    "\n",
    "df_new4.show(truncate=False)\n",
    "\n",
    "#write the result to a new deltatable\n",
    "# df_new.write.format(\"delta\").mode(\"append\").save(\"hdfs:///table_2g_demo\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IO-optimization: Columnar compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean overlap: 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 130:===========>   (35 + 9) / 44][Stage 136:>                (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------------------------------------------------------------------------------------------------------+-------+\n",
      "|ID        |keywords                                                                                                       |overlap|\n",
      "+----------+---------------------------------------------------------------------------------------------------------------+-------+\n",
      "|1133240144|[zakraj, ek, july, ndash, september, slovene, mathematician, computer, scientist, born]                        |1      |\n",
      "|1146534968|[waleswales, identified, inhabited, humans, years, evidenced, discovery, neanderthal, bontnewydd, palaeolithic]|0      |\n",
      "+----------+---------------------------------------------------------------------------------------------------------------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread \"serve-DataFrame\" java.net.SocketTimeoutException: Accept timed out\n",
      "\tat java.net.PlainSocketImpl.socketAccept(Native Method)\n",
      "\tat java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:409)\n",
      "\tat java.net.ServerSocket.implAccept(ServerSocket.java:560)\n",
      "\tat java.net.ServerSocket.accept(ServerSocket.java:528)\n",
      "\tat org.apache.spark.security.SocketAuthServer$$anon$1.run(SocketAuthServer.scala:64)\n"
     ]
    }
   ],
   "source": [
    "#start context and session\n",
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.context import SparkContext\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from delta import *\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "##############\n",
    "#optimization#\n",
    "##############\n",
    "builder = pyspark.sql.SparkSession.builder.appName(\"MyApp\").master('yarn') \\\n",
    "    .config(\"spark.executor.instances\", 12) \\\n",
    "    .config(\"spark.executor.memory\", \"1536m\") \\\n",
    "    .config(\"spark.driver.logLevel\", \"WARN\")\\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .config(\"spark.sql.parquet.compression.codec\", \"snappy\") \\\n",
    "    .config(\"spark.sql.sources.default\", \"delta\")\\\n",
    "    .config(\"spark.shuffle.compress\", \"true\")\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()\n",
    "\n",
    "# This reduces the amount of logs we recieve, only priting ERROR level or higher.\n",
    "sc = spark.sparkContext\n",
    "logger = sc._jvm.org.apache.log4j\n",
    "logger.LogManager.getLogger(\"org\"). setLevel( logger.Level.ERROR )\n",
    "logger.LogManager.getLogger(\"akka\").setLevel( logger.Level.ERROR )\n",
    "sc.setLogLevel('ERROR')\n",
    "\n",
    "#Read in df from deltatable, now compressed\n",
    "##############\n",
    "#optimization#\n",
    "##############\n",
    "dftest = spark.read \\\n",
    "    .format(\"delta\") \\\n",
    "    .option(\"compression\", \"snappy\") \\\n",
    "    .load(\"hdfs:///table_2g\") \\\n",
    "    .limit(2)\n",
    "\n",
    "\n",
    "# convert title and text to an array\n",
    "Ctit = split(lower(regexp_replace(dftest.title, '[^\\w\\s]', '')), \" \")\n",
    "\n",
    "Ctext1 = dftest.select(\"ID\", split(\"text\", \" \").alias(\"filt1\"), Ctit.alias(\"Ctit\"))\n",
    "\n",
    "#filter words in title from the array in filt\n",
    "Ctext2 = Ctext1.select(\"ID\", expr(\"array_except(filt1, Ctit)\").alias(\"filt2\"))\n",
    "\n",
    "#Remove empty strings\n",
    "Ctext3 = Ctext2.select(\"ID\", array_remove(\"filt2\", \"\").alias(\"filt3\"))\n",
    "\n",
    "#Remove stopwords \n",
    "from pyspark.ml.feature import StopWordsRemover # to remove stop words\n",
    "stopwords_remover = StopWordsRemover(inputCol=\"filt3\", outputCol=\"filt4\")\n",
    "Ctext4 = stopwords_remover.transform(Ctext3.select(\"ID\",\"filt3\"))\n",
    "\n",
    "#Count number of documents\n",
    "num_doc = Ctext4.count()\n",
    "\n",
    "#Count number of words in text of each article and create an extra column\n",
    "df_tf1 = Ctext4.select(\"ID\", \"filt4\", size(\"filt4\").alias(\"text_len\"))\n",
    "\n",
    "#Create row for each word in text\n",
    "df_ex = df_tf1.select(\"ID\",\"text_len\", explode(\"filt4\").alias(\"word_split\"))\n",
    "\n",
    "#Find the IDFs for each word\n",
    "import math\n",
    "df_count = df_ex.groupBy(\"word_split\")\\\n",
    "        .agg(log(num_doc/countDistinct(\"ID\")).alias(\"IDF\"))\\\n",
    "        .where(col(\"IDF\") > math.log(num_doc/(num_doc*0.2)))        #filter the words that occur in more than 20% of the documents\n",
    "\n",
    "#Find TF and calculate the TF-IDF\n",
    "df_ex2 = df_ex.groupBy(\"ID\", \"word_split\", \"text_len\") \\\n",
    "        .agg((count(\"word_split\") / col(\"text_len\")).alias(\"TF\"))\n",
    "\n",
    "#Find the IDF for every word using a join and calculate the TF-IDF\n",
    "df_join = df_ex2.join(df_count, on=\"word_split\", how=\"left\")\\\n",
    "                  .select(\"ID\", \"word_split\", expr(\"TF*IDF\").alias(\"TF_IDF\"))\n",
    "\n",
    "#Sort the df by ID\n",
    "from pyspark.sql.window import Window\n",
    "key_part = Window.partitionBy(\"ID\").orderBy(col(\"TF_IDF\").desc())\n",
    "#select the 5 keywords with the highest TF-IDF value \n",
    "df_key = df_join.withColumn(\"row\",row_number().over(key_part))\\\n",
    "                .filter(col(\"row\") <= 10)\\\n",
    "                .drop(\"row\") \\\n",
    "                .groupby('ID')\\\n",
    "                .agg(collect_list('word_split').alias(\"keywords\"))\n",
    "\n",
    "df_new = dftest.select(\"ID\", \"subtitles\", \"related\", \"categories\")\\\n",
    "                .join(df_key.filter(col(\"keywords\").isNotNull()), on=\"ID\", how=\"left\")\n",
    "\n",
    "df_new1 = df_new.select(\"ID\", \"keywords\", concat_ws(\" \", col(\"related\"), col(\"subtitles\"), col(\"categories\")).alias(\"con_col\"))\n",
    "\n",
    "df_new1 = df_new1.filter((df_new1.keywords.isNotNull()) & (df_new1.con_col.isNotNull()))\n",
    "\n",
    "df_new2 = df_new1.select(\"ID\", \"keywords\", lower(\"con_col\").alias(\"con_col_L\"))\n",
    "\n",
    "# #transform to list and make it distinct\n",
    "df_new3 = df_new2.select(\"ID\", \"keywords\", array_distinct(split(\"con_col_L\", \" \")).alias(\"con_split\"))\n",
    "\n",
    "# #compute the overlap\n",
    "df_new4 = df_new3.select(\"ID\", \"keywords\", size(array_intersect(\"con_split\", \"keywords\")).alias(\"overlap\"))\n",
    "\n",
    "# # Compute the mean of the overlap column\n",
    "mean_overlap = df_new4.agg(mean(\"overlap\")).collect()[0][0]\n",
    "\n",
    "# Print the mean value\n",
    "print(\"Mean overlap:\", mean_overlap)\n",
    "\n",
    "df_new4.show(truncate=False)\n",
    "\n",
    "#write the result to a new deltatable\n",
    "# df_new.write.format(\"delta\").mode(\"append\").save(\"hdfs:///table_2g_demo\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Map partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean overlap: 0.5\n",
      "+----------+---------------------------------------------------------------------------------------------------------------+-------+\n",
      "|ID        |keywords                                                                                                       |overlap|\n",
      "+----------+---------------------------------------------------------------------------------------------------------------+-------+\n",
      "|1133240144|[zakraj, ek, july, ndash, september, slovene, mathematician, computer, scientist, born]                        |1      |\n",
      "|1146534968|[waleswales, identified, inhabited, humans, years, evidenced, discovery, neanderthal, bontnewydd, palaeolithic]|0      |\n",
      "+----------+---------------------------------------------------------------------------------------------------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#start context and session\n",
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.context import SparkContext\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from delta import *\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "builder = pyspark.sql.SparkSession.builder.appName(\"MyApp\").master('yarn') \\\n",
    "    .config(\"spark.executor.instances\", 12) \\\n",
    "    .config(\"spark.executor.memory\", \"1536m\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()\n",
    "\n",
    "# This reduces the amount of logs we recieve, only priting ERROR level or higher.\n",
    "sc = spark.sparkContext\n",
    "logger = sc._jvm.org.apache.log4j\n",
    "logger.LogManager.getLogger(\"org\"). setLevel( logger.Level.ERROR )\n",
    "logger.LogManager.getLogger(\"akka\").setLevel( logger.Level.ERROR )\n",
    "sc.setLogLevel('ERROR')\n",
    "\n",
    "#Read in df from deltatable\n",
    "dftest = spark.read.format(\"delta\").load(\"hdfs:///table_2g\").limit(2)  # query table by path\n",
    "\n",
    "# convert title and text to an array\n",
    "Ctit = split(lower(regexp_replace(dftest.title, '[^\\w\\s]', '')), \" \")\n",
    "\n",
    "Ctext1 = dftest.select(\"ID\", split(\"text\", \" \").alias(\"filt1\"), Ctit.alias(\"Ctit\"))\n",
    "\n",
    "#filter words in title from the array in filt\n",
    "Ctext2 = Ctext1.select(\"ID\", expr(\"array_except(filt1, Ctit)\").alias(\"filt2\"))\n",
    "\n",
    "#Remove empty strings\n",
    "Ctext3 = Ctext2.select(\"ID\", array_remove(\"filt2\", \"\").alias(\"filt3\"))\n",
    "\n",
    "#Remove stopwords \n",
    "from pyspark.ml.feature import StopWordsRemover # to remove stop words\n",
    "stopwords_remover = StopWordsRemover(inputCol=\"filt3\", outputCol=\"filt4\")\n",
    "Ctext4 = stopwords_remover.transform(Ctext3.select(\"ID\",\"filt3\"))\n",
    "\n",
    "#Count number of documents\n",
    "num_doc = Ctext4.count()\n",
    "\n",
    "#Count number of words in text of each article and create an extra column\n",
    "df_tf1 = Ctext4.select(\"ID\", \"filt4\", size(\"filt4\").alias(\"text_len\"))\n",
    "\n",
    "#Create row for each word in text\n",
    "df_ex = df_tf1.select(\"ID\",\"text_len\", explode(\"filt4\").alias(\"word_split\"))\n",
    "\n",
    "#Find the IDFs for each word\n",
    "import math\n",
    "df_count = df_ex.groupBy(\"word_split\")\\\n",
    "        .agg(log(num_doc/countDistinct(\"ID\")).alias(\"IDF\"))\\\n",
    "        .where(col(\"IDF\") > math.log(num_doc/(num_doc*0.2)))        #filter the words that occur in more than 20% of the documents\n",
    "\n",
    "#Find TF and calculate the TF-IDF\n",
    "df_ex2 = df_ex.groupBy(\"ID\", \"word_split\", \"text_len\") \\\n",
    "        .agg((count(\"word_split\") / col(\"text_len\")).alias(\"TF\"))\n",
    "\n",
    "#Find the IDF for every word using a join and calculate the TF-IDF\n",
    "df_join = df_ex2.join(df_count, on=\"word_split\", how=\"left\")\\\n",
    "                  .select(\"ID\", \"word_split\", expr(\"TF*IDF\").alias(\"TF_IDF\"))\n",
    "\n",
    "#Sort the df by ID\n",
    "from pyspark.sql.window import Window\n",
    "key_part = Window.partitionBy(\"ID\").orderBy(col(\"TF_IDF\").desc())\n",
    "#select the 5 keywords with the highest TF-IDF value \n",
    "df_key = df_join.withColumn(\"row\",row_number().over(key_part))\\\n",
    "                .filter(col(\"row\") <= 10)\\\n",
    "                .drop(\"row\") \\\n",
    "                .groupby('ID')\\\n",
    "                .agg(collect_list('word_split').alias(\"keywords\"))\n",
    "\n",
    "df_new = dftest.select(\"ID\", \"subtitles\", \"related\", \"categories\")\\\n",
    "                .join(df_key.filter(col(\"keywords\").isNotNull()), on=\"ID\", how=\"left\")\n",
    "\n",
    "df_new1 = df_new.filter((df_new.keywords.isNotNull()))\n",
    "\n",
    "##############\n",
    "#optimization#\n",
    "##############\n",
    "\n",
    "def compute_overlap(partition):\n",
    "    # Define a function to compute the overlap for a single row\n",
    "    def compute_row_overlap(row):\n",
    "        # Compute the concatenated column\n",
    "        con_col = \" \".join(str(val) for val in [row.related, row.subtitles,  row.categories])\n",
    "        sub_split = [word.lower().strip() for word in con_col.split() if word.isalnum()]\n",
    "        overlap = len(set(sub_split).intersection(set(row.keywords)))\n",
    "        return (row.ID, row.keywords, overlap)\n",
    "\n",
    "    # Process each row in the partition using the compute_row_overlap function\n",
    "    return map(compute_row_overlap, partition)\n",
    "\n",
    "# Convert the input DataFrame to an RDD and apply the compute_overlap function\n",
    "df_new2 = df_new1.rdd \\\n",
    "                .mapPartitions(compute_overlap)\\\n",
    "                .toDF([\"ID\", \"keywords\", \"overlap\"])\n",
    "\n",
    "# Convert the resulting RDD to a DataFrame and compute the mean overlap\n",
    "mean_overlap = df_new2.agg(mean(\"overlap\")).collect()[0][0]\n",
    "\n",
    "print(\"Mean overlap:\", mean_overlap)\n",
    "\n",
    "df_new2.show(truncate=False)\n",
    "\n",
    "#write the result to a new deltatable\n",
    "# df_new.write.format(\"delta\").mode(\"overwrite\").save(\"hdfs:///table_2g_extended\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using HOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean overlap: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------------------------------------------------------+-------+\n",
      "|ID        |keywords                                                            |overlap|\n",
      "+----------+--------------------------------------------------------------------+-------+\n",
      "|1146534968|[youth, yl, yeoman, yeast, years, year, y, xiii, x, wrexham]        |0      |\n",
      "|1133240144|[zuse, zakraj, zagreb, z, yugoslavia, wrote, went, vlo, usage, unix]|0      |\n",
      "+----------+--------------------------------------------------------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#start context and session\n",
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.context import SparkContext\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from delta import *\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "builder = pyspark.sql.SparkSession.builder.appName(\"MyApp\").master('yarn') \\\n",
    "    .config(\"spark.executor.instances\", 12) \\\n",
    "    .config(\"spark.executor.memory\", \"1536m\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()\n",
    "\n",
    "# This reduces the amount of logs we recieve, only priting ERROR level or higher.\n",
    "sc = spark.sparkContext\n",
    "logger = sc._jvm.org.apache.log4j\n",
    "logger.LogManager.getLogger(\"org\"). setLevel( logger.Level.ERROR )\n",
    "logger.LogManager.getLogger(\"akka\").setLevel( logger.Level.ERROR )\n",
    "sc.setLogLevel('ERROR')\n",
    "\n",
    "#Read in df from deltatable\n",
    "dftest = spark.read.format(\"delta\").load(\"hdfs:///table_2g\").limit(2)  # query table by path\n",
    "\n",
    "# convert title and text to an array\n",
    "Ctit = split(lower(regexp_replace(dftest.title, '[^\\w\\s]', '')), \" \")\n",
    "\n",
    "Ctext1 = dftest.select(\"ID\", split(\"text\", \" \").alias(\"filt1\"), Ctit.alias(\"Ctit\"))\n",
    "\n",
    "#filter words in title from the array in filt\n",
    "Ctext2 = Ctext1.select(\"ID\", expr(\"array_except(filt1, Ctit)\").alias(\"filt2\"))\n",
    "\n",
    "#Remove empty strings\n",
    "Ctext3 = Ctext2.select(\"ID\", array_remove(\"filt2\", \"\").alias(\"filt3\"))\n",
    "\n",
    "#Remove stopwords \n",
    "from pyspark.ml.feature import StopWordsRemover # to remove stop words\n",
    "stopwords_remover = StopWordsRemover(inputCol=\"filt3\", outputCol=\"filt4\")\n",
    "Ctext4 = stopwords_remover.transform(Ctext3.select(\"ID\",\"filt3\"))\n",
    "\n",
    "#Count number of documents\n",
    "num_doc = Ctext4.count()\n",
    "\n",
    "# Apply the transformation and create a new column with the resulting array of {word, T} arrays\n",
    "expr1 = \"array_distinct(filt4)\"\n",
    "df_tf1 = Ctext4.select(\"ID\", expr(expr1).alias(\"word_dis\"), size(\"filt4\").alias(\"text_len\"))\n",
    "\n",
    "expr2 = \"transform(word_dis, x -> named_struct('word', x, 'tf', size(filter(word_dis, y -> y = x ))/ text_len))\"\n",
    "df_tf2 = df_tf1.select(\"ID\", \"word_dis\", \"text_len\", expr(expr2).alias('TF_array'))\n",
    "\n",
    "# create a df with for every distinct word in an article a row, and calculate IDF in one pass\n",
    "import math\n",
    "df_count = Ctext4.selectExpr(\"ID\", \"explode(filt4) as word_split\")\\\n",
    "        .groupBy(\"word_split\")\\\n",
    "        .agg(log(num_doc/countDistinct(\"ID\")).alias(\"IDF\"))#\\\n",
    "        #.where(col(\"IDF\") > math.log(num_doc/(num_doc*0.2)))  \n",
    "\n",
    "lookup_map = df_count.groupBy()\\\n",
    "              .agg(map_from_entries(collect_set(struct(\"word_split\", \"IDF\"))).alias(\"idf_map\"))\\\n",
    "              .selectExpr(\"idf_map\")\\\n",
    "              .collect()[0][\"idf_map\"]\n",
    "\n",
    "# Define a UDF that maps words to their IDF values using a dictionary\n",
    "def map_idf(lookup_dict):\n",
    "    def inner_map_idf(word_dis):\n",
    "        return [lookup_dict[word] for word in word_dis]\n",
    "    return udf(inner_map_idf, ArrayType(DoubleType()))\n",
    "\n",
    "# Apply the UDF to the DataFrame\n",
    "df_tf3 = df_tf2.select(\"ID\", \"TF_array\", map_idf(lookup_map)(col(\"word_dis\")).alias(\"IDF\"))\n",
    "\n",
    "#Calculate the TF-IDF\n",
    "df_tf4 = df_tf3.select(\"ID\", expr(\"transform(arrays_zip(TF_array, IDF), x -> named_struct('tf_idf', x.TF_array.tf * x.IDF, 'word', x.TF_array.word ))\").alias(\"tf_idf\"))\n",
    "\n",
    "#Sort the words according to the TF*IDF score and select the first 5 words\n",
    "df_tf5 = df_tf4.select(\"ID\", expr(\"sort_array(tf_idf, False)\").alias(\"sorted\"))\n",
    "df_key = df_tf5.select(\"ID\", slice(expr(\"transform(sorted, x -> x.word)\"), 1, 10).alias(\"keywords\"))\n",
    "\n",
    "df_new = dftest.select(\"ID\", \"subtitles\", \"related\", \"categories\")\\\n",
    "                .join(df_key.filter(col(\"keywords\").isNotNull()), on=\"ID\", how=\"left\")\n",
    "\n",
    "df_new1 = df_new.select(\"ID\", \"keywords\", concat_ws(\" \", col(\"related\"), col(\"subtitles\"), col(\"categories\")).alias(\"con_col\"))\n",
    "\n",
    "df_new1 = df_new1.filter((df_new1.keywords.isNotNull()) & (df_new1.con_col.isNotNull()))\n",
    "\n",
    "#transform to list and make it distinct\n",
    "df_new2 = df_new1.select(\"ID\", \"keywords\", array_distinct(split(\"con_col\", \" \")).alias(\"con_split\"))\n",
    "\n",
    "#compute the overlap\n",
    "df_new3 = df_new2.select(\"ID\", \"keywords\", size(array_intersect(\"con_split\", \"keywords\")).alias(\"overlap\"))\n",
    "\n",
    "# Compute the mean of the overlap column\n",
    "mean_overlap = df_new3.agg(mean(\"overlap\")).collect()[0][0]\n",
    "\n",
    "# Print the mean value\n",
    "print(\"Mean overlap:\", mean_overlap)\n",
    "\n",
    "df_new3.show(truncate=False)\n",
    "\n",
    "#write the result to a new deltatable\n",
    "# df_new.write.format(\"delta\").mode(\"append\").save(\"hdfs:///table_2g_demo\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
